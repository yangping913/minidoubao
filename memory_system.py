# import json
# import faiss
# import numpy as np
# from datetime import datetime
# from dateparser import parse
# from sentence_transformers import SentenceTransformer
# from sklearn.cluster import KMeans
# import requests
# from typing import List, Dict, Any
# import logging
#
# # é…ç½®æ—¥å¿—
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)
#
#
# class UniversalMemorySystem:
#     def __init__(self):
#         self.memory_store = []
#         self.memory_embeddings = None
#         self.index = None
#         self.last_accessed = {}
#
#         # Ollama é…ç½®
#         self.ollama_url = "http://localhost:11434/api"
#         self.ollama_embedding_model = "nomic-embed-text"  # æ¨èç”¨äºåµŒå…¥çš„æ¨¡å‹
#         self.ollama_chat_model = "llama2"  # é»˜è®¤èŠå¤©æ¨¡å‹
#         self.ollama_timeout = 10  # è¯·æ±‚è¶…æ—¶æ—¶é—´
#
#         # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
#         try:
#             # ä½¿ç”¨æ¸…åé•œåƒæºåŠ è½½æ¨¡å‹
#             self.model = SentenceTransformer(
#                 'paraphrase-multilingual-MiniLM-L12-v2',
#                 cache_folder='./models',
#                 mirror='https://mirrors.tuna.tsinghua.edu.cn/huggingface-models'
#             )
#             logger.info("âœ…  SentenceTransformer æ¨¡å‹åŠ è½½æˆåŠŸ")
#         except Exception as e:
#             logger.error(f"âŒ  SentenceTransformer æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
#             # ä½¿ç”¨å¤‡ç”¨æ¨¡å‹
#             try:
#                 self.model = SentenceTransformer('distiluse-base-multilingual-cased-v2')
#                 logger.info("âœ…  ä½¿ç”¨å¤‡ç”¨åµŒå…¥æ¨¡å‹")
#             except Exception as e2:
#                 logger.error(f"âŒ  å¤‡ç”¨æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {e2}")
#                 self.model = None
#
#     def add_memory(self, content, source="user", timestamp=None, importance=1.0):
#         """æ·»åŠ æ–°è®°å¿†åˆ°è®°å¿†ç³»ç»Ÿ
#
#         Args:
#             content (str): è®°å¿†å†…å®¹
#             source (str): è®°å¿†æ¥æºï¼Œé»˜è®¤ä¸º"user"
#             timestamp (str): æ—¶é—´æˆ³ï¼Œé»˜è®¤ä¸ºå½“å‰æ—¶é—´
#             importance (float): è®°å¿†é‡è¦æ€§æƒé‡ï¼ŒèŒƒå›´0.1-3.0
#
#         Returns:
#             dict: æ·»åŠ çš„è®°å¿†å¯¹è±¡
#         """
#         if not timestamp:
#             timestamp = datetime.now().isoformat()
#
#         memory = {
#             "id": len(self.memory_store),
#             "content": content,
#             "source": source,
#             "timestamp": timestamp,
#             "last_accessed": timestamp,
#             "access_count": 0,
#             "importance": max(0.1, min(3.0, importance)),  # é™åˆ¶é‡è¦æ€§èŒƒå›´
#             "cluster": -1,
#             "embedding_type": "unknown"  # æ ‡è®°åµŒå…¥ç”Ÿæˆæ–¹å¼
#         }
#
#         self.memory_store.append(memory)
#         self._update_index(memory)
#         return memory
#
#     def _update_index(self, memory):
#         """æ›´æ–°FAISSç´¢å¼•ï¼Œä¼˜å…ˆä½¿ç”¨Ollamaç”ŸæˆåµŒå…¥
#
#         Args:
#             memory (dict): è¦ç´¢å¼•çš„è®°å¿†å¯¹è±¡
#         """
#         try:
#             # ä¼˜å…ˆå°è¯•ä½¿ç”¨Ollamaç”ŸæˆåµŒå…¥
#             embedding = None
#             if self._check_ollama_available():
#                 embeddings = self.ollama_embed([memory["content"]])
#                 if embeddings and embeddings[0]:
#                     embedding = embeddings[0]
#                     memory["embedding_type"] = "ollama"
#                     logger.info("âœ…  ä½¿ç”¨Ollamaç”ŸæˆåµŒå…¥")
#
#             # å¦‚æœOllamaä¸å¯ç”¨æˆ–å¤±è´¥ï¼Œä½¿ç”¨åŸæ¨¡å‹
#             if embedding is None and self.model:
#                 embedding = self.model.encode([memory["content"]])[0]
#                 memory["embedding_type"] = "sentence_transformer"
#                 logger.info("âœ…  ä½¿ç”¨SentenceTransformerç”ŸæˆåµŒå…¥")
#
#             # å¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ
#             if embedding is None:
#                 embedding = self._fallback_embedding(memory["content"])
#                 memory["embedding_type"] = "fallback"
#                 logger.warning("âš ï¸  ä½¿ç”¨é™çº§åµŒå…¥æ–¹æ¡ˆ")
#
#             # æ›´æ–°ç´¢å¼•
#             if self.memory_embeddings is None:
#                 self.memory_embeddings = np.array([embedding])
#                 self.index = faiss.IndexFlatL2(embedding.shape[0])
#                 self.index.add(self.memory_embeddings.astype('float32'))
#             else:
#                 new_embedding = np.array([embedding]).astype('float32')
#                 self.memory_embeddings = np.vstack([self.memory_embeddings, embedding])
#                 self.index.add(new_embedding)
#
#         except Exception as e:
#             logger.error(f"âŒ  æ›´æ–°ç´¢å¼•å¤±è´¥: {e}")
#
#     def get_ollama_models(self) -> List[str]:
#         """è·å–æœ¬åœ°å¯ç”¨çš„Ollamaæ¨¡å‹åˆ—è¡¨
#
#         Returns:
#             List[str]: å¯ç”¨çš„æ¨¡å‹åç§°åˆ—è¡¨
#         """
#         try:
#             response = requests.get(
#                 f"{self.ollama_url}/tags",
#                 timeout=self.ollama_timeout
#             )
#             if response.status_code == 200:
#                 models = [model['name'] for model in response.json().get('models', [])]
#                 logger.info(f"âœ…  è·å–åˆ° {len(models)} ä¸ªOllamaæ¨¡å‹")
#                 return models
#             return []
#         except requests.exceptions.RequestException as e:
#             logger.warning(f"âš ï¸  è·å–Ollamaæ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
#             return []
#         except Exception as e:
#             logger.error(f"âŒ  è·å–Ollamaæ¨¡å‹åˆ—è¡¨å¼‚å¸¸: {e}")
#             return []
#
#     def _check_ollama_available(self) -> bool:
#         """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨
#
#         Returns:
#             bool: OllamaæœåŠ¡æ˜¯å¦å¯ç”¨
#         """
#         try:
#             response = requests.get(
#                 f"{self.ollama_url}/tags",
#                 timeout=3
#             )
#             return response.status_code == 200
#         except:
#             return False
#
#     def ollama_embed(self, texts: List[str]) -> List[List[float]]:
#         """ä½¿ç”¨Ollamaç”ŸæˆåµŒå…¥å‘é‡
#
#         Args:
#             texts (List[str]): è¦åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨
#
#         Returns:
#             List[List[float]]: åµŒå…¥å‘é‡åˆ—è¡¨
#         """
#         if not self._check_ollama_available():
#             logger.warning("âš ï¸  OllamaæœåŠ¡ä¸å¯ç”¨")
#             return []
#
#         embeddings = []
#         for text in texts:
#             try:
#                 data = {
#                     "model": self.ollama_embedding_model,
#                     "prompt": text
#                 }
#                 response = requests.post(
#                     f"{self.ollama_url}/embeddings",
#                     json=data,
#                     timeout=self.ollama_timeout
#                 )
#
#                 if response.status_code == 200:
#                     embedding_data = response.json()
#                     embedding = embedding_data.get('embedding', [])
#                     if embedding:
#                         embeddings.append(embedding)
#                     else:
#                         logger.warning("âš ï¸  Ollamaè¿”å›ç©ºåµŒå…¥")
#                         embeddings.append(self._fallback_embedding(text))
#                 else:
#                     logger.warning(f"âš ï¸  OllamaåµŒå…¥è¯·æ±‚å¤±è´¥: {response.status_code}")
#                     embeddings.append(self._fallback_embedding(text))
#
#             except requests.exceptions.Timeout:
#                 logger.warning("âš ï¸  OllamaåµŒå…¥è¯·æ±‚è¶…æ—¶")
#                 embeddings.append(self._fallback_embedding(text))
#             except requests.exceptions.RequestException as e:
#                 logger.warning(f"âš ï¸  Ollamaç½‘ç»œé”™è¯¯: {e}")
#                 embeddings.append(self._fallback_embedding(text))
#             except Exception as e:
#                 logger.error(f"âŒ  OllamaåµŒå…¥å¼‚å¸¸: {e}")
#                 embeddings.append(self._fallback_embedding(text))
#
#         return embeddings
#
#     def _fallback_embedding(self, text: str) -> List[float]:
#         """ç®€å•çš„é™çº§åµŒå…¥æ–¹æ¡ˆ
#
#         Args:
#             text (str): è¾“å…¥æ–‡æœ¬
#
#         Returns:
#             List[float]: é™çº§åµŒå…¥å‘é‡
#         """
#         # åŸºäºæ–‡æœ¬é•¿åº¦å’Œå­—ç¬¦åˆ†å¸ƒçš„ç®€å•åµŒå…¥
#         text_length = len(text)
#         word_count = len(text.split())
#         char_diversity = len(set(text)) / max(1, len(text))
#
#         # ç”Ÿæˆ384ç»´çš„åµŒå…¥ï¼ˆä¸å¸¸è§åµŒå…¥ç»´åº¦åŒ¹é…ï¼‰
#         embedding = []
#         np.random.seed(hash(text) % 10000)  # åŸºäºæ–‡æœ¬å†…å®¹çš„ç¡®å®šæ€§éšæœº
#
#         # å‰10ç»´åŸºäºæ–‡æœ¬ç‰¹å¾
#         embedding.append(text_length / 1000.0)  # å½’ä¸€åŒ–é•¿åº¦
#         embedding.append(word_count / 200.0)  # å½’ä¸€åŒ–è¯æ•°
#         embedding.append(char_diversity)  # å­—ç¬¦å¤šæ ·æ€§
#         embedding.append(sum(1 for c in text if c.isdigit()) / max(1, text_length))  # æ•°å­—æ¯”ä¾‹
#         embedding.append(sum(1 for c in text if c.isalpha()) / max(1, text_length))  # å­—æ¯æ¯”ä¾‹
#
#         # å‰©ä½™ç»´åº¦ä½¿ç”¨éšæœºå€¼ï¼ˆä½†ä¿æŒç¡®å®šæ€§ï¼‰
#         for i in range(379):
#             embedding.append(np.random.uniform(-0.1, 0.1))
#
#         return embedding
#
#     def should_use_local_model(self, query: str) -> bool:
#         """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨æœ¬åœ°æ¨¡å‹å¤„ç†æŸ¥è¯¢
#
#         Args:
#             query (str): ç”¨æˆ·æŸ¥è¯¢å†…å®¹
#
#         Returns:
#             bool: æ˜¯å¦æ¨èä½¿ç”¨æœ¬åœ°æ¨¡å‹
#         """
#         if not query or not isinstance(query, str):
#             return False
#
#         query_lower = query.lower()
#         query_length = len(query)
#
#         # å®šä¹‰å…³é”®è¯è§„åˆ™
#         local_keywords = ["ä½ å¥½", "è¯·é—®", "ä»‹ç»", "è§£é‡Š", "ä»€ä¹ˆæ˜¯", "æ€ä¹ˆæ ·",
#                           "å‘Šè¯‰æˆ‘", "å»ºè®®", "æ¨è", "å¦‚ä½•", "æ€æ ·", "èƒ½ä¸èƒ½"]
#         remote_keywords = ["æœ€æ–°", "2024", "2025", "æ–°é—»", "æ›´æ–°", "æœ€è¿‘",
#                            "å®æ—¶", "å½“å‰", "ç°åœ¨", "ä»Šå¤©", "æœ¬å‘¨", "æœ¬æœˆ"]
#
#         complex_keywords = ["ä»£ç ", "ç¼–ç¨‹", "ç®—æ³•", "æ•°å­¦", "è®¡ç®—", "ç†è®º",
#                             "ç ”ç©¶", "åˆ†æ", "æ¯”è¾ƒ", "è¯„ä¼°", "æ€»ç»“", "è®ºè¿°"]
#
#         # åŒ…å«è¿œç¨‹å…³é”®è¯åˆ™ä½¿ç”¨DeepSeekï¼ˆéœ€è¦æœ€æ–°çŸ¥è¯†ï¼‰
#         for keyword in remote_keywords:
#             if keyword in query_lower:
#                 logger.info(f"ğŸ”  æ£€æµ‹åˆ°è¿œç¨‹å…³é”®è¯ '{keyword}'ï¼Œæ¨èä½¿ç”¨DeepSeek")
#                 return False
#
#         # éå¸¸çŸ­çš„æŸ¥è¯¢ä¼˜å…ˆä½¿ç”¨æœ¬åœ°
#         if query_length < 10:
#             logger.info("ğŸ”  çŸ­æŸ¥è¯¢ï¼Œæ¨èä½¿ç”¨æœ¬åœ°æ¨¡å‹")
#             return True
#
#         # ä¸­ç­‰é•¿åº¦æŸ¥è¯¢ï¼ˆ10-50å­—ç¬¦ï¼‰æ ¹æ®å…³é”®è¯åˆ¤æ–­
#         if 10 <= query_length <= 50:
#             for keyword in local_keywords:
#                 if keyword in query_lower:
#                     logger.info(f"ğŸ”  æ£€æµ‹åˆ°æœ¬åœ°å…³é”®è¯ '{keyword}'ï¼Œæ¨èä½¿ç”¨æœ¬åœ°æ¨¡å‹")
#                     return True
#
#             for keyword in complex_keywords:
#                 if keyword in query_lower:
#                     logger.info(f"ğŸ”  æ£€æµ‹åˆ°å¤æ‚å…³é”®è¯ '{keyword}'ï¼Œæ¨èä½¿ç”¨DeepSeek")
#                     return False
#
#         # é•¿æŸ¥è¯¢ï¼ˆ>50å­—ç¬¦ï¼‰é€šå¸¸éœ€è¦æ›´å¼ºçš„æ¨¡å‹
#         if query_length > 50:
#             logger.info("ğŸ”  é•¿æŸ¥è¯¢ï¼Œæ¨èä½¿ç”¨DeepSeek")
#             return False
#
#         # é»˜è®¤ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆèŠ‚çœAPIè°ƒç”¨ï¼‰
#         logger.info("ğŸ”  é»˜è®¤æ¨èä½¿ç”¨æœ¬åœ°æ¨¡å‹")
#         return True
#
#     def retrieve_memories(self, query, top_k=5, recency_weight=0.3, importance_weight=0.5):
#         """æ£€ç´¢ç›¸å…³è®°å¿†ï¼Œæ”¯æŒå¤šç§æƒé‡å¹³è¡¡
#
#         Args:
#             query (str): æŸ¥è¯¢æ–‡æœ¬
#             top_k (int): è¿”å›æœ€å¤šè®°å¿†æ•°é‡
#             recency_weight (float): æ—¶æ•ˆæ€§æƒé‡
#             importance_weight (float): é‡è¦æ€§æƒé‡
#
#         Returns:
#             List[dict]: ç›¸å…³è®°å¿†åˆ—è¡¨ï¼ŒæŒ‰ç›¸å…³æ€§æ’åº
#         """
#         if not self.memory_store or not query:
#             return []
#
#         # ç¡®ä¿æƒé‡æ€»å’Œä¸è¶…è¿‡1.0
#         similarity_weight = 1.0 - recency_weight - importance_weight
#         if similarity_weight < 0:
#             similarity_weight = 0.5
#             recency_weight = 0.25
#             importance_weight = 0.25
#             logger.warning("âš ï¸  æƒé‡è°ƒæ•´ï¼Œä½¿ç”¨é»˜è®¤æƒé‡é…ç½®")
#
#         try:
#             # ç”ŸæˆæŸ¥è¯¢åµŒå…¥ï¼ˆä¼˜å…ˆä½¿ç”¨Ollamaï¼‰
#             query_embedding = None
#             if self._check_ollama_available():
#                 ollama_embeddings = self.ollama_embed([query])
#                 if ollama_embeddings and ollama_embeddings[0]:
#                     query_embedding = ollama_embeddings[0]
#
#             if query_embedding is None and self.model:
#                 query_embedding = self.model.encode([query])[0]
#
#             if query_embedding is None:
#                 query_embedding = self._fallback_embedding(query)
#
#             # æœç´¢ç›¸ä¼¼è®°å¿†
#             distances, indices = self.index.search(
#                 np.array([query_embedding]).astype('float32'),
#                 min(top_k * 2, len(self.memory_store))  # å¤šæ£€ç´¢ä¸€äº›ç”¨äºåç»­ç­›é€‰
#             )
#
#             # è®¡ç®—ç»¼åˆåˆ†æ•°
#             results = []
#             for i, idx in enumerate(indices[0]):
#                 if idx < len(self.memory_store):
#                     memory = self.memory_store[idx].copy()
#
#                     # è®¡ç®—æ—¶æ•ˆæ€§åˆ†æ•°
#                     try:
#                         last_accessed_dt = parse(memory["last_accessed"])
#                         if isinstance(last_accessed_dt, datetime):
#                             days_since_access = (datetime.now() - last_accessed_dt).days
#                             recency = 1.0 / (1.0 + days_since_access)
#                         else:
#                             recency = 0.5
#                     except:
#                         recency = 0.5
#
#                     # è®¡ç®—ç›¸ä¼¼æ€§åˆ†æ•°ï¼ˆè·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼‰
#                     similarity_score = 1.0 / (1.0 + distances[0][i])
#
#                     # è®¡ç®—ç»¼åˆåˆ†æ•°
#                     memory["score"] = (
#                             similarity_score * similarity_weight +
#                             recency * recency_weight +
#                             memory["importance"] * importance_weight
#                     )
#
#                     # æ·»åŠ è¯¦ç»†è¯„åˆ†ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
#                     memory["score_breakdown"] = {
#                         "similarity": similarity_score,
#                         "recency": recency,
#                         "importance": memory["importance"],
#                         "weights": {
#                             "similarity": similarity_weight,
#                             "recency": recency_weight,
#                             "importance": importance_weight
#                         }
#                     }
#
#                     results.append(memory)
#
#             # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›å‰top_kä¸ª
#             results.sort(key=lambda x: x["score"], reverse=True)
#
#             # æ›´æ–°è®¿é—®ä¿¡æ¯
#             for memory in results[:top_k]:
#                 self.update_memory_access(memory["id"])
#
#             return results[:top_k]
#
#         except Exception as e:
#             logger.error(f"âŒ  è®°å¿†æ£€ç´¢å¤±è´¥: {e}")
#             # é™çº§å¤„ç†ï¼šè¿”å›æœ€è¿‘çš„é‡è¦è®°å¿†
#             return self._fallback_retrieve(top_k)
#
#     def _fallback_retrieve(self, top_k=5):
#         """é™çº§è®°å¿†æ£€ç´¢æ–¹æ¡ˆ"""
#         recent_memories = sorted(
#             self.memory_store,
#             key=lambda x: (x["importance"], parse(x["last_accessed"])),
#             reverse=True
#         )
#         return recent_memories[:top_k]
#
#     def update_memory_access(self, memory_id):
#         """æ›´æ–°è®°å¿†è®¿é—®ä¿¡æ¯"""
#         if memory_id < len(self.memory_store):
#             memory = self.memory_store[memory_id]
#             memory["last_accessed"] = datetime.now().isoformat()
#             memory["access_count"] += 1
#
#     def cluster_memories(self, n_clusters=5):
#         """èšç±»ç›¸å…³è®°å¿†"""
#         if len(self.memory_store) < n_clusters or self.memory_embeddings is None:
#             for memory in self.memory_store:
#                 memory["cluster"] = -1
#             return
#
#         try:
#             kmeans = KMeans(
#                 n_clusters=min(n_clusters, len(self.memory_store)),
#                 random_state=0,
#                 n_init=10  # æ˜ç¡®è®¾ç½®n_initä»¥é¿å…è­¦å‘Š
#             )
#             clusters = kmeans.fit_predict(self.memory_embeddings)
#
#             for i, label in enumerate(clusters):
#                 self.memory_store[i]["cluster"] = int(label)
#
#         except Exception as e:
#             logger.error(f"âŒ  è®°å¿†èšç±»å¤±è´¥: {e}")
#             for memory in self.memory_store:
#                 memory["cluster"] = -1
#
#     def summarize_context(self, query, max_tokens=500):
#         """ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦"""
#         relevant_memories = self.retrieve_memories(query, top_k=10)
#
#         if not relevant_memories:
#             return "æš‚æ— ç›¸å…³è®°å¿†"
#
#         # æŒ‰ä¸»é¢˜åˆ†ç»„è®°å¿†
#         topics = {}
#         for memory in relevant_memories:
#             cluster = memory["cluster"]
#             if cluster not in topics:
#                 topics[cluster] = []
#             topics[cluster].append(memory)
#
#         # ç”Ÿæˆä¸»é¢˜æ‘˜è¦
#         summaries = []
#         for cluster, memories in topics.items():
#             memories.sort(key=lambda x: x["importance"] * x["access_count"], reverse=True)
#             cluster_summary = "ã€".join([m["content"][:50] + "..." for m in memories[:3]])
#             cluster_label = f"ä¸»é¢˜{cluster + 1}" if cluster >= 0 else "æœªåˆ†ç±»ä¸»é¢˜"
#             summaries.append(f"{cluster_label}: {cluster_summary}")
#
#         full_summary = "\n".join(summaries)[:max_tokens]
#         logger.info(f"ğŸ“  ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦ï¼Œé•¿åº¦: {len(full_summary)}")
#         return full_summary
#
#     def extract_key_info(self, text):
#         """ä»æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡æ¯ï¼ˆç”¨æˆ·åã€AIåç§°ç­‰ï¼‰"""
#         key_info = {}
#
#         if not text or not isinstance(text, str):
#             return key_info
#
#         # æå–ç”¨æˆ·å§“å
#         name_keywords = ["æˆ‘å«", "æˆ‘æ˜¯", "ä½ å¯ä»¥å«æˆ‘", "æˆ‘çš„åå­—æ˜¯", "ç§°å‘¼æˆ‘"]
#         for keyword in name_keywords:
#             if keyword in text:
#                 start_index = text.index(keyword) + len(keyword)
#                 rest_text = text[start_index:]
#
#                 # æŸ¥æ‰¾ç»“æŸä½ç½®
#                 end_chars = ["ã€‚", "ï¼Œ", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼š", ".", ",", "!", "?", ";", ":", " "]
#                 end_index = len(rest_text)
#                 for char in end_chars:
#                     if char in rest_text:
#                         char_index = rest_text.index(char)
#                         if char_index < end_index:
#                             end_index = char_index
#
#                 name = rest_text[:end_index].strip()
#                 if 2 <= len(name) <= 4 and all(c.isalpha() or c.isspace() for c in name):
#                     key_info["user_name"] = name
#                     logger.info(f"ğŸ‘¤  æå–åˆ°ç”¨æˆ·å: {name}")
#                     break
#
#         # æå–AIåç§°
#         ai_name_keywords = ["ä½ å°±å«", "ä½ çš„åå­—æ˜¯", "æˆ‘ç§°ä½ ä¸º", "ä½ å°±ç§°å‘¼", "ä½ çš„åç§°"]
#         for keyword in ai_name_keywords:
#             if keyword in text:
#                 start_index = text.index(keyword) + len(keyword)
#                 rest_text = text[start_index:]
#
#                 end_chars = ["ã€‚", "ï¼Œ", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼š", ".", ",", "!", "?", ";", ":", " "]
#                 end_index = len(rest_text)
#                 for char in end_chars:
#                     if char in rest_text:
#                         char_index = rest_text.index(char)
#                         if char_index < end_index:
#                             end_index = char_index
#
#                 name = rest_text[:end_index].strip()
#                 if 2 <= len(name) <= 6:
#                     key_info["ai_name"] = name
#                     logger.info(f"ğŸ¤–  æå–åˆ°AIåç§°: {name}")
#                     break
#
#         return key_info
#
#     def get_memory_stats(self):
#         """è·å–è®°å¿†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
#         if not self.memory_store:
#             return {
#                 "total_memories": 0,
#                 "embedding_types": {},
#                 "avg_importance": 0,
#                 "avg_access_count": 0
#             }
#
#         embedding_types = {}
#         total_importance = 0
#         total_access = 0
#
#         for memory in self.memory_store:
#             embed_type = memory.get("embedding_type", "unknown")
#             embedding_types[embed_type] = embedding_types.get(embed_type, 0) + 1
#             total_importance += memory.get("importance", 1.0)
#             total_access += memory.get("access_count", 0)
#
#         return {
#             "total_memories": len(self.memory_store),
#             "embedding_types": embedding_types,
#             "avg_importance": total_importance / len(self.memory_store),
#             "avg_access_count": total_access / len(self.memory_store)
#         }
#
#     def clear_memories(self):
#         """æ¸…ç©ºæ‰€æœ‰è®°å¿†ï¼ˆç”¨äºæµ‹è¯•æˆ–é‡ç½®ï¼‰"""
#         self.memory_store = []
#         self.memory_embeddings = None
#         self.index = None
#         logger.warning("âš ï¸  å·²æ¸…ç©ºæ‰€æœ‰è®°å¿†")